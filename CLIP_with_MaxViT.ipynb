{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "gogyMPbAi6lQ",
        "uM65_Loji6lW",
        "MeHbWHfqa1sM"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mid6342/Vision-Language-Models-with-MaxViT/blob/master/CLIP_with_MaxViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gogyMPbAi6lQ"
      },
      "source": [
        "## Installs, imports and download of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSvBQoszjYJS"
      },
      "source": [
        "!pip install timm\n",
        "!pip install transformers\n",
        "!pip install pytorch_metric_learning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 3.862689,
          "end_time": "2021-04-05T08:01:49.835804",
          "exception": false,
          "start_time": "2021-04-05T08:01:45.973115",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "bvXzfQqgi6lT"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "from tqdm.autonotebook import tqdm\n",
        "import albumentations as A\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer\n",
        "\n",
        "from pytorch_metric_learning import losses\n",
        "from torch import einsum, nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2y6l5An9jIxI"
      },
      "source": [
        "!pip install kaggle --upgrade\n",
        "os.environ['KAGGLE_USERNAME'] = \"XXX\"\n",
        "os.environ['KAGGLE_KEY'] = \"XXX\"\n",
        "\n",
        "### For Flickr 8k\n",
        "!kaggle datasets download -d adityajn105/flickr8k\n",
        "!unzip flickr8k.zip\n",
        "dataset = \"8k\"\n",
        "\n",
        "### For Flickr 30k\n",
        "# !kaggle datasets download -d hsankesara/flickr-image-dataset\n",
        "# !unzip flickr-image-dataset.zip\n",
        "# dataset = \"30k\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 1.449907,
          "end_time": "2021-04-05T08:01:51.298515",
          "exception": false,
          "start_time": "2021-04-05T08:01:49.848608",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "ieBwPS2Qi6lT"
      },
      "source": [
        "if dataset == \"8k\":\n",
        "  df = pd.read_csv(\"captions.txt\")\n",
        "  df['id'] = [id_ for id_ in range(df.shape[0] // 5) for _ in range(5)]\n",
        "  df.to_csv(\"captions.csv\", index=False)\n",
        "  df = pd.read_csv(\"captions.csv\")\n",
        "  image_path = \"/content/Images\"\n",
        "  captions_path = \"/content\"\n",
        "elif dataset == \"30k\":\n",
        "  df = pd.read_csv(\"/content/flickr30k_images/results.csv\", delimiter=\"|\")\n",
        "  df.columns = ['image', 'caption_number', 'caption']\n",
        "  df['caption'] = df['caption'].str.lstrip()\n",
        "  df['caption_number'] = df['caption_number'].str.lstrip()\n",
        "  df.loc[19999, 'caption_number'] = \"4\"\n",
        "  df.loc[19999, 'caption'] = \"A dog runs across the grass .\"\n",
        "  ids = [id_ for id_ in range(len(df) // 5) for _ in range(5)]\n",
        "  df['id'] = ids\n",
        "  df.to_csv(\"captions.csv\", index=False)\n",
        "  image_path = \"/content/flickr30k_images/flickr30k_images\"\n",
        "  captions_path = \"/content\"\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.012732,
          "end_time": "2021-04-05T08:01:51.324144",
          "exception": false,
          "start_time": "2021-04-05T08:01:51.311412",
          "status": "completed"
        },
        "tags": [],
        "id": "o54bL4gji6lU"
      },
      "source": [
        "## Configuration of the hyperparameters and the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.377383,
          "end_time": "2021-04-05T08:01:51.714313",
          "exception": false,
          "start_time": "2021-04-05T08:01:51.336930",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "9ulGHg9ai6lV"
      },
      "source": [
        "class CFG:\n",
        "    debug = False\n",
        "    image_path = image_path\n",
        "    captions_path = captions_path\n",
        "    batch_size = 16 \n",
        "    \n",
        "    # best setup of hyperparameters after hyperparametertuning with raytune\n",
        "    head_lr = 0.00312827\n",
        "    image_encoder_lr = 1.43573e-05\n",
        "    text_encoder_lr = 5.1619e-06\n",
        "    weight_decay = 0.00126294\n",
        "\n",
        "\n",
        "    num_workers = 2\n",
        "    patience = 1\n",
        "    factor = 0.8\n",
        "    epochs = 5\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model_name = 'maxvit_rmlp_small_rw_224' \n",
        "    image_embedding = 768 \n",
        "    text_encoder_model = \"distilbert-base-uncased\"\n",
        "    text_embedding = 768\n",
        "    text_tokenizer = \"distilbert-base-uncased\"\n",
        "    max_length = 200\n",
        "\n",
        "    pretrained = True # for both image encoder and text encoder\n",
        "    trainable = True # for both image encoder and text encoder\n",
        "    temperature = 1.0\n",
        "\n",
        "    # image size\n",
        "    size = 224 \n",
        "\n",
        "    # for projection head; used for both image and text encoders\n",
        "    num_projection_layers = 1\n",
        "    projection_dim = 256 \n",
        "    dropout = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.023459,
          "end_time": "2021-04-05T08:01:51.776328",
          "exception": false,
          "start_time": "2021-04-05T08:01:51.752869",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "_piLd4dxi6lV"
      },
      "source": [
        "class AvgMeter:\n",
        "    def __init__(self, name=\"Metric\"):\n",
        "        self.name = name\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.avg, self.sum, self.count = [0] * 3\n",
        "\n",
        "    def update(self, val, count=1):\n",
        "        self.count += count\n",
        "        self.sum += val * count\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __repr__(self):\n",
        "        text = f\"{self.name}: {self.avg:.4f}\"\n",
        "        return text\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group[\"lr\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.025532,
          "end_time": "2021-04-05T08:01:51.840523",
          "exception": false,
          "start_time": "2021-04-05T08:01:51.814991",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "l9V91XcNi6lW"
      },
      "source": [
        "class CLIPDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_filenames, captions, tokenizer, transforms):\n",
        "        \"\"\"\n",
        "        image_filenames and cpations must have the same length; so, if there are\n",
        "        multiple captions for each image, the image_filenames must have repetitive\n",
        "        file names \n",
        "        \"\"\"\n",
        "\n",
        "        self.image_filenames = image_filenames\n",
        "        self.captions = list(captions)\n",
        "        self.encoded_captions = tokenizer(\n",
        "            list(captions), padding=True, truncation=True, max_length=CFG.max_length\n",
        "        )\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {\n",
        "            key: torch.tensor(values[idx])\n",
        "            for key, values in self.encoded_captions.items()\n",
        "        }\n",
        "\n",
        "        image = cv2.imread(f\"{CFG.image_path}/{self.image_filenames[idx]}\")\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = self.transforms(image=image)['image']\n",
        "        item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n",
        "        item['caption'] = self.captions[idx]\n",
        "\n",
        "        return item\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.captions)\n",
        "\n",
        "\n",
        "\n",
        "def get_transforms(mode=\"train\"):\n",
        "    if mode == \"train\":\n",
        "        return A.Compose(\n",
        "            [\n",
        "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
        "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
        "            ]\n",
        "        )\n",
        "    else:\n",
        "        return A.Compose(\n",
        "            [\n",
        "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
        "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
        "            ]\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.041512,
          "end_time": "2021-04-05T08:01:52.054352",
          "exception": false,
          "start_time": "2021-04-05T08:01:52.012840",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "HUSC32Cei6ld"
      },
      "source": [
        "def make_train_valid_dfs():\n",
        "    dataframe = pd.read_csv(f\"{CFG.captions_path}/captions.csv\")\n",
        "    max_id = dataframe[\"id\"].max() + 1 if not CFG.debug else 100\n",
        "    image_ids = np.arange(0, max_id)\n",
        "    np.random.seed(42)\n",
        "    valid_and_test_ids = np.random.choice(\n",
        "        image_ids, size=int(2000), replace=False \n",
        "    )\n",
        "    valid_ids = np.random.choice(\n",
        "        valid_and_test_ids, size=int(0.5 * len(valid_and_test_ids)), replace=False\n",
        "    )\n",
        "    test_ids = [id_ for id_ in valid_and_test_ids if id_ not in valid_ids]\n",
        "    train_ids = [id_ for id_ in image_ids if id_ not in valid_and_test_ids]\n",
        "    train_dataframe = dataframe[dataframe[\"id\"].isin(train_ids)].reset_index(drop=True)\n",
        "    valid_dataframe = dataframe[dataframe[\"id\"].isin(valid_ids)].reset_index(drop=True)\n",
        "    test_dataframe = dataframe[dataframe[\"id\"].isin(test_ids)].reset_index(drop=True)\n",
        "    return train_dataframe, valid_dataframe, test_dataframe\n",
        "\n",
        "\n",
        "def build_loaders(dataframe, tokenizer, mode):\n",
        "    transforms = get_transforms(mode=mode)\n",
        "    dataset = CLIPDataset(\n",
        "        dataframe[\"image\"].values,\n",
        "        dataframe[\"caption\"].values,\n",
        "        tokenizer=tokenizer,\n",
        "        transforms=transforms,\n",
        "    )\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=CFG.batch_size,\n",
        "        num_workers=CFG.num_workers,\n",
        "        drop_last=True,\n",
        "        shuffle=True if mode == \"train\" else False,\n",
        "    )\n",
        "    return dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.012817,
          "end_time": "2021-04-05T08:01:51.802043",
          "exception": false,
          "start_time": "2021-04-05T08:01:51.789226",
          "status": "completed"
        },
        "tags": [],
        "id": "uM65_Loji6lW"
      },
      "source": [
        "## Vision-language model and loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.027706,
          "end_time": "2021-04-05T08:01:51.907283",
          "exception": false,
          "start_time": "2021-04-05T08:01:51.879577",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "i0flfMTRi6lY"
      },
      "source": [
        "class ImageEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encode images to a fixed size vector\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, model_name=CFG.model_name, pretrained=CFG.pretrained, trainable=CFG.trainable\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model(\n",
        "            model_name, pretrained, num_classes=0, global_pool=\"avg\"\n",
        "        )\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = trainable\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.027706,
          "end_time": "2021-04-05T08:01:51.907283",
          "exception": false,
          "start_time": "2021-04-05T08:01:51.879577",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "am5VR4Ezi6lZ"
      },
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable):\n",
        "        super().__init__()\n",
        "        if pretrained:\n",
        "            self.model = DistilBertModel.from_pretrained(model_name)\n",
        "        else:\n",
        "            self.model = DistilBertModel(config=DistilBertConfig())\n",
        "            \n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = trainable\n",
        "\n",
        "        # we are using the CLS token hidden representation as the sentence's embedding\n",
        "        self.target_token_idx = 0\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = output.last_hidden_state\n",
        "        return last_hidden_state[:, self.target_token_idx, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.027706,
          "end_time": "2021-04-05T08:01:51.907283",
          "exception": false,
          "start_time": "2021-04-05T08:01:51.879577",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "SJbY9Yrui6la"
      },
      "source": [
        "class ProjectionHead(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dim,\n",
        "        projection_dim=CFG.projection_dim,\n",
        "        dropout=CFG.dropout\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        projected = self.projection(x)\n",
        "        x = self.gelu(projected)\n",
        "        x = self.fc(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x + projected\n",
        "        x = self.layer_norm(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.025366,
          "end_time": "2021-04-05T08:01:51.972338",
          "exception": false,
          "start_time": "2021-04-05T08:01:51.946972",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "7MQnmwsWi6lc"
      },
      "source": [
        "class CLIPModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        temperature=CFG.temperature,\n",
        "        image_embedding=CFG.image_embedding,\n",
        "        text_embedding=CFG.text_embedding,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.image_encoder = ImageEncoder()\n",
        "        self.text_encoder = TextEncoder()\n",
        "        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n",
        "        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, batch):\n",
        "        # Getting Image and Text Features\n",
        "        image_features = self.image_encoder(batch[\"image\"])\n",
        "        text_features = self.text_encoder(\n",
        "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
        "        )\n",
        "        # Getting Image and Text Embeddings (with same dimension)\n",
        "        image_embeddings = self.image_projection(image_features)\n",
        "        text_embeddings = self.text_projection(text_features)\n",
        "\n",
        "        ce = F.cross_entropy\n",
        "        sim = einsum('i d, j d -> i j', text_embeddings, image_embeddings)\n",
        "        #sim = sim * self.temperature.exp()\n",
        "        contrastive_labels = torch.arange(CFG.batch_size).cuda()\n",
        "\n",
        "        contrastive_loss = (ce(sim, contrastive_labels) + ce(sim.t(), contrastive_labels)) * 0.5\n",
        "\n",
        "        return contrastive_loss.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "MeHbWHfqa1sM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.041512,
          "end_time": "2021-04-05T08:01:52.054352",
          "exception": false,
          "start_time": "2021-04-05T08:01:52.012840",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "Jkjp0BEPi6le"
      },
      "source": [
        "def train_epoch(model, train_loader, optimizer, lr_scheduler, step):\n",
        "    loss_meter = AvgMeter()\n",
        "    tqdm_object = tqdm(train_loader, total=len(train_loader))\n",
        "    for batch in tqdm_object:\n",
        "        batch = {k: v.to(CFG.device) for k, v in batch.items() if k != \"caption\"}\n",
        "        loss = model(batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if step == \"batch\":\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        count = batch[\"image\"].size(0)\n",
        "        loss_meter.update(loss.item(), count)\n",
        "\n",
        "        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))\n",
        "    return loss_meter\n",
        "\n",
        "\n",
        "def valid_epoch(model, valid_loader):\n",
        "    loss_meter = AvgMeter()\n",
        "\n",
        "    image_features = []\n",
        "    text_features = []\n",
        "    tqdm_object = tqdm(valid_loader, total=len(valid_loader))\n",
        "    for batch in tqdm_object:\n",
        "        batch = {k: v.to(CFG.device) for k, v in batch.items() if k != \"caption\"}\n",
        "        loss = model(batch)\n",
        "\n",
        "        count = batch[\"image\"].size(0)\n",
        "        loss_meter.update(loss.item(), count)\n",
        "\n",
        "        tqdm_object.set_postfix(valid_loss=loss_meter.avg)\n",
        "        \n",
        "        image_features_ = model.image_encoder(batch[\"image\"].to(CFG.device))\n",
        "        image_embeds = model.image_projection(image_features_)\n",
        "        \n",
        "      \n",
        "        text_features_ = model.text_encoder(\n",
        "        input_ids=batch[\"input_ids\"].to(CFG.device), attention_mask=batch[\"attention_mask\"].to(CFG.device)\n",
        "        )\n",
        "        text_embeds = model.text_projection(text_features_)\n",
        "\n",
        "        image_features.append(image_embeds.detach().cpu())\n",
        "        text_features.append(text_embeds.detach().cpu())\n",
        "\n",
        "    m = get_metrics(torch.cat(image_features), torch.cat(text_features), 1)\n",
        "    print(m)\n",
        "\n",
        "\n",
        "    r_performance = m[\"image_to_text_mean_rank\"] + m[\"image_to_text_median_rank\"] + m[\"text_to_image_mean_rank\"] + m[\"text_to_image_median_rank\"]\n",
        "    print(\"Combined Mean and Median Retrieval: \", r_performance)\n",
        "\n",
        "    \n",
        "    print(\"valid_loss: \", loss_meter)\n",
        "\n",
        "    return loss_meter, r_performance\n",
        "\n",
        "def main():\n",
        "    train_df, valid_df, test_df = make_train_valid_dfs()\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n",
        "    train_loader = build_loaders(train_df, tokenizer, mode=\"train\")\n",
        "    valid_loader = build_loaders(valid_df, tokenizer, mode=\"train\")\n",
        "    test_loader = build_loaders(test_df, tokenizer, mode=\"test\")\n",
        "\n",
        "\n",
        "    model = CLIPModel().to(CFG.device)\n",
        "    params = [\n",
        "        {\"params\": model.image_encoder.parameters(), \"lr\": CFG.image_encoder_lr},\n",
        "        {\"params\": model.text_encoder.parameters(), \"lr\": CFG.text_encoder_lr},\n",
        "        {\"params\": itertools.chain(\n",
        "            model.image_projection.parameters(), model.text_projection.parameters()\n",
        "        ), \"lr\": CFG.head_lr, \"weight_decay\": CFG.weight_decay}\n",
        "    ]\n",
        "    optimizer = torch.optim.AdamW(params, weight_decay=0.)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode=\"min\", patience=CFG.patience, factor=CFG.factor\n",
        "    )\n",
        "    step = \"epoch\"\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    best_performance = float('inf')\n",
        "    for epoch in range(CFG.epochs):\n",
        "        print(f\"Epoch: {epoch + 1}\")\n",
        "        model.train()\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, lr_scheduler, step)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            valid_loss, r_performance = valid_epoch(model, valid_loader)\n",
        "\n",
        "        \n",
        "        if r_performance < best_performance: \n",
        "            best_loss = valid_loss.avg\n",
        "            best_performance = r_performance\n",
        "            torch.save(model.state_dict(), \"/content/best.pt\")\n",
        "            print(\"Saved Best Model!\")\n",
        "        \n",
        "        lr_scheduler.step(valid_loss.avg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_metrics(image_features, text_features, logit_scale):\n",
        "    metrics = {}\n",
        "    logits_per_image = (logit_scale * image_features @ text_features.t()).detach().cpu()\n",
        "    logits_per_text = logits_per_image.t().detach().cpu()\n",
        "\n",
        "    logits = {\"image_to_text\": logits_per_image, \"text_to_image\": logits_per_text}\n",
        "    ground_truth = torch.arange(len(text_features)).view(-1, 1)\n",
        "\n",
        "    for name, logit in logits.items():\n",
        "        ranking = torch.argsort(logit, descending=True)\n",
        "        preds = torch.where(ranking == ground_truth)[1]\n",
        "        preds = preds.numpy() #.detach()\n",
        "        metrics[f\"{name}_mean_rank\"] = preds.mean() + 1\n",
        "        metrics[f\"{name}_median_rank\"] = np.floor(np.median(preds)) + 1\n",
        "        for k in [1, 5, 10]:\n",
        "            metrics[f\"{name}_R@{k}\"] = np.mean(preds < k)\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "PULxKgAWA5DW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 16449.265031,
          "end_time": "2021-04-05T12:36:01.333760",
          "exception": false,
          "start_time": "2021-04-05T08:01:52.068729",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "2G3S841Vi6le"
      },
      "source": [
        "main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiMjVEVci6le"
      },
      "source": [
        "# Evaluation and test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_embeddings(test_df, model_path):\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n",
        "    test_loader = build_loaders(test_df, tokenizer, mode=\"test\")\n",
        "    \n",
        "    model = CLIPModel().to(CFG.device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=CFG.device))\n",
        "    model.eval()\n",
        "    \n",
        "    test_image_embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader):\n",
        "            image_features = model.image_encoder(batch[\"image\"].to(CFG.device))\n",
        "            image_embeddings = model.image_projection(image_features)\n",
        "            test_image_embeddings.append(image_embeddings)\n",
        "    return model, torch.cat(test_image_embeddings)"
      ],
      "metadata": {
        "id": "TCqp5cqn8Zwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySr-oGpMi6lf"
      },
      "source": [
        "_, _, test_df = make_train_valid_dfs()\n",
        "\n",
        "model, image_embeddings = get_image_embeddings(test_df, \"/content/best.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n",
        "image_featuress = []\n",
        "text_featuress = []\n",
        "valid_loader = build_loaders(test_df, tokenizer, mode=\"valid\")\n",
        "#tqdm_object = tqdm(test_loader, total=len(test_loader))\n",
        "with torch.no_grad():\n",
        "  for batch in tqdm(valid_loader):\n",
        "        image_features = model.image_encoder(batch[\"image\"].to(CFG.device))\n",
        "        image_embeds = model.image_projection(image_features)\n",
        "        \n",
        "        text_features_ = model.text_encoder(\n",
        "        input_ids=batch[\"input_ids\"].to(CFG.device), attention_mask=batch[\"attention_mask\"].to(CFG.device)\n",
        "        )\n",
        "        text_embeds = model.text_projection(text_features_)\n",
        "\n",
        "        image_featuress.append(image_embeds.detach().cpu())\n",
        "        text_featuress.append(text_embeds.detach().cpu())\n",
        "\n",
        "m = get_metrics(torch.cat(image_featuress), torch.cat(text_featuress), 1)\n",
        "print(m)"
      ],
      "metadata": {
        "id": "bvJwh_fI_t8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP81I_wxi6lf"
      },
      "source": [
        "### Image retrieval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.025647,
          "end_time": "2021-04-05T12:36:01.385717",
          "exception": false,
          "start_time": "2021-04-05T12:36:01.360070",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "un6RzF4Ri6lg"
      },
      "source": [
        "def find_matches(model, image_embeddings, query, image_filenames, n):\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n",
        "    encoded_query = tokenizer([query])\n",
        "    batch = {\n",
        "        key: torch.tensor(values).to(CFG.device)\n",
        "        for key, values in encoded_query.items()\n",
        "    }\n",
        "    with torch.no_grad():\n",
        "        text_features = model.text_encoder(\n",
        "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
        "        )\n",
        "        text_embeddings = model.text_projection(text_features)\n",
        "    \n",
        "    image_embeddings_n = F.normalize(image_embeddings, p=2, dim=-1)\n",
        "    text_embeddings_n = F.normalize(text_embeddings, p=2, dim=-1)\n",
        "    dot_similarity = text_embeddings_n @ image_embeddings_n.T\n",
        "    \n",
        "    values, indices = torch.topk(dot_similarity.squeeze(0), n * 5)\n",
        "    matches = [image_filenames[idx] for idx in indices[::5]]\n",
        "    \n",
        "    _, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
        "    for match, ax in zip(matches, axes.flatten()):\n",
        "        image = cv2.imread(f\"{CFG.image_path}/{match}\")\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        ax.imshow(image)\n",
        "        ax.axis(\"off\")\n",
        "    \n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "waXVzbioi6lg"
      },
      "source": [
        "find_matches(model, \n",
        "             image_embeddings,\n",
        "             query=\"dog on the beach\",\n",
        "             image_filenames=test_df['image'].values,\n",
        "             n=9)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}